{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "acc01a1f313476b1649f26c880e6c69cee6e9491"
      },
      "cell_type": "code",
      "source": "#Includes L2 and dropout regularization.\n#https://www.kaggle.com/dsoreo/two-layer-neural-network-with-tensorflow",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf #For tenserflow neuralnetwork\nimport sklearn.preprocessing #to make one hot encoding matrix\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "train_data = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")\n\n#print(\"Sample Train data\\n\",train_data.head())\n#print(\"Sample Test data\\n\", test_data.head())\nprint(\"Train shape\",train_data.shape)\nprint(\"Test shape\", test_data.shape)\n\ntrain_data  = train_data.sample(train_data.shape[0],random_state=1).reset_index(drop=True)\n#print(\"Sample Train data\\n\",train_data.head())\nprint(\"Train shape\",train_data.shape)\n\nnrow = 38500\ntrain_train = train_data.iloc[0:nrow,]\ntrain_val = train_data.iloc[(nrow):,]\nprint(\"Train Train shape\",train_train.shape)\nprint(\"Train validation shape\",train_val.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8da9cb99902f1f69b93d3f056b3d16006ec44799"
      },
      "cell_type": "code",
      "source": "train_Y = train_train.iloc[:,0]\ntrain_X = train_train.iloc[:,1:]\n\nval_Y = train_val.iloc[:,0]\nval_X = train_val.iloc[:,1:]\n\n#print(train_Y.shape, train_X.shape,val_Y.shape, val_X.shape)\n\ntrain_Y = np.array(train_Y).reshape(train_Y.shape[0],1)\nval_Y = np.array(val_Y).reshape(val_Y.shape[0],1)\ntrain_X = np.array(train_X)\nval_X = np.array(val_X)\ntest_X = np.array(test_data)\n\n#print(train_Y.shape, train_X.shape,val_Y.shape, val_X.shape)\n\ntrain_X = np.transpose(train_X)\ntrain_X = train_X.astype(np.float32)\ntrain_Y = np.transpose(train_Y)\n\nval_X = np.transpose(val_X)\nval_X = val_X.astype(np.float32)\nval_Y = np.transpose(val_Y)\n\ntest_X = np.transpose(test_X)\ntest_X = test_X.astype(np.float32)\n\nprint(\"train_y_shape:\",train_Y.shape, \"train_x_shape:\",train_X.shape,\"val_y_shape:\",val_Y.shape, \"val_x_shape:\",val_X.shape,\n     \"Test_X_shape\",test_X.shape)\n#del train_data, test_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09187858cf64f264059b872afe9771ea1a3cd984"
      },
      "cell_type": "code",
      "source": "#Create placeholders for X and Y\ndef create_placeholder (n_x, n_y):\n    X = tf.placeholder(tf.float32, shape=[n_x,None])\n    Y = tf.placeholder(tf.float32, shape=[n_y,None])\n    return X,Y\nprint(\"Done...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5af44886d22194bd6f948f4e7f6c24c0975ae11d"
      },
      "cell_type": "code",
      "source": "def initialize_parameters (n_x, n_y, n_layer1, n_layer2):\n    W1 = tf.get_variable(\"W1\",[n_layer1,n_x],initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b1 = tf.get_variable(\"b1\",[n_layer1,1],initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\",[n_layer2,n_layer1],initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b2 = tf.get_variable(\"b2\",[n_layer2,1],initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\",[n_y,n_layer2],initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b3 = tf.get_variable(\"b3\",[n_y,1],initializer = tf.zeros_initializer())\n    \n    parameters = {\"W1\": W1,\"b1\": b1,\"W2\": W2,\"b2\": b2,\"W3\": W3,\"b3\": b3}\n    return parameters\nprint(\"Done...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f28d858ea20bce88c6455365d6f503f29ca0a334"
      },
      "cell_type": "code",
      "source": "def forward_prop(X, parameters,keep_prob=0.9, dropout=False):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    W3 = parameters['W3']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    b3 = parameters['b3']\n    \n    Z1 = tf.add(tf.matmul(W1,X),b1, name=\"Z1\")\n    A1 = tf.nn.tanh(Z1, name=\"A1\")\n    if dropout==True:\n        A1 = tf.nn.dropout(A1, keep_prob)\n    Z2 = tf.add(tf.matmul(W2,A1),b2,name=\"Z2\")\n    A2 = tf.nn.tanh(Z2,name=\"A2\")\n    if dropout==True:\n        A2 = tf.nn.dropout(A2, keep_prob)\n    Z3 = tf.add(tf.matmul(W3,A2),b3,name=\"Z3\")\n    \n    return Z3\nprint(\"Done...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2dc5d6021dd347b8f3dac51014693cf0bfcfb792"
      },
      "cell_type": "code",
      "source": "def get_cost (Z3, Y, regularizer, reg_param=0.001, l2=False):\n    prediction = tf.transpose(Z3)\n    actual = tf.transpose(Y)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=actual))\n    if l2==True:\n        cost = tf.reduce_mean(cost+reg_param * regularizer)\n    return cost\nprint(\"Done...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62ac9633960b52cb4a7f5641ae3dbedcc771c97a"
      },
      "cell_type": "code",
      "source": "def get_regularizer(parameters):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    W3 = parameters['W3']\n    regularizer = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)\n    return regularizer\nprint('Done...')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bf5d2f6f82e5d546cec6387bfceb597ec4facb8"
      },
      "cell_type": "code",
      "source": "#https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\ndef convertToOneHot(vector, num_classes=None):\n    assert isinstance(vector, np.ndarray)\n    assert len(vector) > 0\n\n    if num_classes is None:\n        num_classes = np.max(vector)+1\n    else:\n        assert num_classes > 0\n        assert num_classes >= np.max(vector)\n\n    result = np.zeros(shape=(len(vector), num_classes))\n    result[np.arange(len(vector)), vector] = 1\n    return result.astype(int)\nprint(\"Done...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa7624636efcf02f3bf495fb2fa6c3e4f6f4c879"
      },
      "cell_type": "code",
      "source": "n_x = train_X.shape[0]\nm = train_X.shape[1]\nn_y = np.unique(train_Y).shape[0] #Total softmax unit entries\nn_layer1 = 30\nn_layer2 = 25\nlearning_rate = 0.001\nminibatch_size = 1024\nn_epoch = 200\nreg_coef = 0.002\nkeep_prob = 0.9\n\n#Create one hot encoding for Y\nY_train_oh = np.transpose(convertToOneHot(np.transpose(train_Y).reshape(train_Y.shape[1]),10))\n#print(Y_train_oh.shape)\nY_val_oh = np.transpose(convertToOneHot(np.transpose(val_Y).reshape(val_Y.shape[1]),10))\n#print(Y_val_oh.shape)\n\ntf.reset_default_graph()\nX,Y = create_placeholder(n_x,n_y)\nparam = initialize_parameters(n_x, n_y, n_layer1, n_layer2)\n#Set the last parameter to true to build NN with dropout regularization.\nZ3 =  forward_prop(X, param, keep_prob, True)\nregularizer = get_regularizer(param)\n#Set the last parameter to true to build NN with L2 regularization.\nf_cost = get_cost(Z3, Y, regularizer, reg_coef,False)\n\n#Backward propogation\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(f_cost)\nprint('Done...')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50045d7d2bdeee9e9144c778ee4c17e167c2e7d6"
      },
      "cell_type": "code",
      "source": "#Start running tensorflow\ninit = tf.global_variables_initializer()\nseed = 3\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(n_epoch):\n        epoch_cost = 0\n        num_minibatches = int(m / minibatch_size)\n        \n        for nb in range(num_minibatches):\n            seed = seed+1\n            randcol = np.random.randint(0,m,minibatch_size)\n            minibatch_x = train_X[:,randcol]\n            minibatch_y = Y_train_oh[:,randcol]\n            _ , minibatch_cost = sess.run([optimizer, f_cost], feed_dict={X: minibatch_x, Y: minibatch_y})\n            epoch_cost += minibatch_cost / num_minibatches\n        \n        # Print the cost every epoch\n        if epoch % 25 == 0:\n            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n        \n    parameters = sess.run(param)\n    correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        \n    print (\"Train Accuracy:\", accuracy.eval({X: train_X, Y: Y_train_oh}))\n    print (\"Validation Accuracy:\", accuracy.eval({X: val_X, Y: Y_val_oh}))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c45e6da7a34477cb4201068d8c7ee50a1ccdc086"
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    sess.run(init)\n    predictions_test = tf.argmax(forward_prop(test_X,parameters))\n    predictions = sess.run(predictions_test)\n    print(predictions.shape)\n    print(predictions[1:50])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "28b94e8b896b5ff1e10d5825d76ccc0851bb109c"
      },
      "cell_type": "code",
      "source": "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nprint(sample_submission.head())\nsample_submission.Label = predictions\nprint(sample_submission.head())\nsample_submission.to_csv('submission.csv',sep=\",\",index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}